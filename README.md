# neural_network_from_scratch
implementation of some of the most common neural network layers (LinearLayer, ReLU, LReLU, BatchNorm, Dropout) from scratch (forward and backward methods) and some optimizers (SGD, momentum, NAG, AdaGrad, RMSProp, Adam)
